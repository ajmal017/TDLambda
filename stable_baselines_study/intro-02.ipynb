{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模型的保存与读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 忽略warning错误信息\n",
    "\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines import A2C, SAC, PPO2, TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre saved (array([-0.5466813], dtype=float32), None)\n",
      "loaded (array([-0.5466813], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# 建立用于保存模型的目录\n",
    "save_dir = \"./save/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 建立模型\n",
    "model = PPO2('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
    "# 保存模型 PPO2_tutorial.zip\n",
    "model.save(save_dir + \"/PPO2_tutorial\")\n",
    "\n",
    "\n",
    "# 验证读取模型的一致性\n",
    "# sample an observation from the environment\n",
    "obs = model.env.observation_space.sample()\n",
    "\n",
    "# Check prediction before saving\n",
    "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
    "\n",
    "del model # delete trained model to demonstrate loading\n",
    "\n",
    "loaded_model = PPO2.load(save_dir + \"/PPO2_tutorial\")\n",
    "# Check that the prediction is the same after loading (for the same observation)\n",
    "print(\"loaded\", loaded_model.predict(obs, deterministic=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存和读取模型最大的好处是，可以边训练、边保存，再读取继续训练模型。防止内存数据溢出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tx/anaconda3/envs/rl4ast/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tx/anaconda3/envs/rl4ast/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tx/anaconda3/envs/rl4ast/lib/python3.6/site-packages/stable_baselines/a2c/a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tx/anaconda3/envs/rl4ast/lib/python3.6/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "loaded: gamma = 0.9 n_steps = 20\n",
      "---------------------------------\n",
      "| explained_variance | 0.00385  |\n",
      "| fps                | 95       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 20       |\n",
      "| value_loss         | 3.26e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0332  |\n",
      "| fps                | 984      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 1.59e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.012   |\n",
      "| fps                | 999      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 1.93e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0265  |\n",
      "| fps                | 1009     |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 982      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00594 |\n",
      "| fps                | 1004     |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 2.02e+03 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.a2c.a2c.A2C at 0x7f108c6435f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 建立用于保存模型的目录\n",
    "save_dir = \"./save/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 定义模型\n",
    "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "# 保存模型 A2C_tutorial.zip\n",
    "model.save(save_dir + \"/A2C_tutorial\")\n",
    "\n",
    "# 删除原模型\n",
    "del model \n",
    "\n",
    "# 加载保存的模型\n",
    "loaded_model = A2C.load(save_dir + \"/A2C_tutorial\", verbose=1)\n",
    "\n",
    "# show the save hyperparameters\n",
    "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n",
    "\n",
    "# 将模型与环境连接\n",
    "# as the environment is not serializable, we need to set a new instance of the environment\n",
    "loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n",
    "# and continue training\n",
    "loaded_model.learn(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gym and VecEnv wrappers\n",
    "\n",
    "A gym wrapper follows the gym interface: it has a reset() and step() method. Because a wrapper is around an environment, we can access it with self.env, this allow to easily interact with it without modifying the original env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(CustomWrapper, self).__init__(env)\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    obs = self.env.reset()\n",
    "    return obs\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 First example: limit the episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  :param max_steps: (int) Max number of steps per episode\n",
    "  \"\"\"\n",
    "  def __init__(self, env, max_steps=100):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(TimeLimitWrapper, self).__init__(env)\n",
    "    self.max_steps = max_steps # 定义最大步长\n",
    "    # Counter of steps per episode\n",
    "    self.current_step = 0\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    self.current_step = 0\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    self.current_step += 1\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    # Overwrite the done signal when \n",
    "    if self.current_step >= self.max_steps:\n",
    "          done = True\n",
    "          # Update the info dict to signal that the limit was exceeded\n",
    "          info['time_limit_reached'] = True\n",
    "    return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 {'time_limit_reached': True}\n"
     ]
    }
   ],
   "source": [
    "# 测试环境\n",
    "from gym.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
    "env = PendulumEnv()\n",
    "# Wrap the environment\n",
    "env = TimeLimitWrapper(env, max_steps=100)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done:\n",
    "    random_action = env.action_space.sample()\n",
    "    # 达到最大步长后停止，在env.step（）函数中控制\n",
    "    obs, reward, done, info = env.step(random_action)\n",
    "    env.render()\n",
    "    n_steps += 1\n",
    "    \n",
    "env.close()\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Second example: normalize actions\n",
    "\n",
    "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent hard to debug issue.\n",
    "\n",
    "In this example, we are going to normalize the action space of Pendulum-v0 so it lies in [-1, 1] instead of [-2, 2].\n",
    "\n",
    "Note: here we are dealing with continuous actions, hence the gym.Box space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # Retrieve the action space\n",
    "        action_space = env.action_space\n",
    "        # 确保是连续的动作空间\n",
    "        assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "        \n",
    "        # Retrieve the max/min values\n",
    "        self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "        # We modify the action space, so all actions will lie in [-1, 1]\n",
    "        env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
    "\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(NormalizeActionWrapper, self).__init__(env)\n",
    "  \n",
    "    def rescale_action(self, scaled_action):\n",
    "        \"\"\"\n",
    "          Rescale the action from [-1, 1] to [low, high]\n",
    "          (no need for symmetric action space)\n",
    "          :param scaled_action: (np.ndarray)\n",
    "          :return: (np.ndarray)\n",
    "          \"\"\"\n",
    "        return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment \n",
    "        \"\"\"\n",
    "        # Reset the counter\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "        \"\"\"\n",
    "        # Rescale action from [-1, 1] to original [low, high] interval\n",
    "        rescaled_action = self.rescale_action(action)\n",
    "        obs, reward, done, info = self.env.step(rescaled_action)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.]\n",
      "[0.20659406]\n",
      "[1.5130978]\n",
      "[-1.0943062]\n",
      "[1.9629356]\n",
      "[1.7582728]\n",
      "[-0.05007163]\n",
      "[0.99274355]\n",
      "[1.3490527]\n",
      "[-1.7977108]\n",
      "[0.24460626]\n",
      "[-1.]\n",
      "[0.2625217]\n",
      "[-0.30736285]\n",
      "[0.805443]\n",
      "[0.38870916]\n",
      "[-0.5412025]\n",
      "[0.7188048]\n",
      "[-0.3701165]\n",
      "[0.09230722]\n",
      "[-0.94432396]\n",
      "[0.28304732]\n"
     ]
    }
   ],
   "source": [
    "# 测试环境\n",
    "\n",
    "# 原来的环境\n",
    "original_env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(original_env.action_space.low)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())\n",
    "\n",
    "# 新的环境\n",
    "env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n",
    "\n",
    "print(env.action_space.low)\n",
    "for _ in range(10):\n",
    "      print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Monitor wrapper \n",
    "\n",
    "We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| explained_variance | -0.182   |\n",
      "| fps                | 30       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 113      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0308   |\n",
      "| fps                | 614      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 500      |\n",
      "| value_loss         | 653      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0416  |\n",
      "| fps                | 703      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 447      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "org_env = gym.make('Pendulum-v0')\n",
    "\n",
    "# 原环境下的训练输出\n",
    "model = A2C(\"MlpPolicy\", org_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | -0.00195 |\n",
      "| fps                | 29       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 971      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 200       |\n",
      "| ep_reward_mean     | -1.75e+03 |\n",
      "| explained_variance | -0.206    |\n",
      "| fps                | 639       |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 1.42      |\n",
      "| total_timesteps    | 500       |\n",
      "| value_loss         | 107       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 200       |\n",
      "| ep_reward_mean     | -1.42e+03 |\n",
      "| explained_variance | -0.00183  |\n",
      "| fps                | 703       |\n",
      "| nupdates           | 200       |\n",
      "| policy_entropy     | 1.42      |\n",
      "| total_timesteps    | 1000      |\n",
      "| value_loss         | 407       |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Monitor Wrapper 下的输出\n",
    "mon_env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
    "mon_env = DummyVecEnv([lambda: mon_env])\n",
    "model = A2C(\"MlpPolicy\", mon_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | -0.0417  |\n",
      "| fps                | 29       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.42     |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 287      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 200       |\n",
      "| ep_reward_mean     | -1.52e+03 |\n",
      "| explained_variance | -0.0161   |\n",
      "| fps                | 626       |\n",
      "| nupdates           | 100       |\n",
      "| policy_entropy     | 1.42      |\n",
      "| total_timesteps    | 500       |\n",
      "| value_loss         | 96.9      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| ep_len_mean        | 200       |\n",
      "| ep_reward_mean     | -1.34e+03 |\n",
      "| explained_variance | -0.00161  |\n",
      "| fps                | 688       |\n",
      "| nupdates           | 200       |\n",
      "| policy_entropy     | 1.42      |\n",
      "| total_timesteps    | 1000      |\n",
      "| value_loss         | 496       |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 可以用多个wrappers的嵌套\n",
    "normalized_env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
    "# Note that we can use multiple wrappers\n",
    "normalized_env = NormalizeActionWrapper(normalized_env)\n",
    "normalized_env = DummyVecEnv([lambda: normalized_env])\n",
    "\n",
    "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Additional wrappers: VecEnvWrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Environments\n",
    "Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Because of this, actions passed to the environment are now a vector (of dimension n). It is the same for observations, rewards and end of episode signals (dones). In the case of non-array observation spaces such as Dict or Tuple, where different sub-spaces may have different shapes, the sub-observations are vectors (of dimension n).\n",
    "\n",
    "* DummyVecEnv: Creates a simple vectorized wrapper for multiple environments, calling each environment in sequence on the current Python process. \n",
    "* VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n",
    "* VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. sucessive frame of an atari game)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00257274  0.00693956  0.00756877]] [-1.9999806]\n",
      "[[-0.98675806 -0.82780082  0.99958868]] [-1.287048]\n",
      "[[-1.26605007 -1.28255063  1.18649636]] [-1.0246431]\n",
      "[[-1.39662169 -1.50355055  1.19014693]] [-0.8883706]\n",
      "[[-1.47944287 -1.66787636  1.35452686]] [-0.7907782]\n",
      "[[-1.50824391 -1.78664143  1.41903748]] [-0.7410711]\n",
      "[[-1.46904282 -1.850043    1.30965066]] [-0.7049484]\n",
      "[[-1.37155844 -1.88683722  1.27379552]] [-0.65967]\n",
      "[[-1.2010548  -1.89022432  1.15042482]] [-0.62446564]\n",
      "[[-0.94812426 -1.86178195  0.94591632]] [-0.54811007]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v0\")])\n",
    "normalized_vec_env = VecNormalize(env)\n",
    "\n",
    "obs = normalized_vec_env.reset()\n",
    "for _ in range(10):\n",
    "    action = [normalized_vec_env.action_space.sample()]\n",
    "    obs, reward, _, _ = normalized_vec_env.step(action)\n",
    "    print(obs, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
